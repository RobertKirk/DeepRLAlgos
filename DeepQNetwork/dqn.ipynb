{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of Deep Q-Learning\n",
    "As described in [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf), published in Nature, 26 February 2015. Research performed by DeepMind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robert/repos/DeepRLAlgos\n"
     ]
    }
   ],
   "source": [
    "cd /home/robert/repos/DeepRLAlgos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from time import sleep\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "from skimage.transform import rescale\n",
    "\n",
    "from common.algorithm import RLAlgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", [\"init_state\", \"act\", \"after_state\", \"reward\", \"terminal\"])\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self._size = size\n",
    "        self._storage = []\n",
    "        self._len = self._index = 0\n",
    "    \n",
    "    def store(self, init_state, act, after_state, reward, terminal):\n",
    "        if self._len == self._size:\n",
    "            self._storage[self._index] = Experience(init_state, act, after_state, reward, terminal)\n",
    "            self._index %= self._size\n",
    "        else:\n",
    "            self._storage.append(Experience(init_state, act, after_state, reward, terminal))\n",
    "            self._len = self._index = self._index + 1\n",
    "    \n",
    "    def sample(self, n_samples=1):\n",
    "        sample_indexes = np.random.choice(len(self._storage), n_samples, replace=False, )\n",
    "        return [self._storage[i] for i in sample_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv_3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.linear_1 = nn.Linear(in_features=3456, out_features=512)\n",
    "        self.head = nn.Linear(in_features=512, out_features=action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        obs = F.relu(self.conv_1(obs))\n",
    "        obs = F.relu(self.conv_2(obs))\n",
    "        obs = F.relu(self.conv_3(obs))\n",
    "        obs = obs.view(obs.size(0), -1) # resizing image to flat for linear\n",
    "        obs = F.relu(self.linear_1(obs))\n",
    "        return self.head(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(RLAlgorithm):\n",
    "    def __init__(self, env, random_seed=100, learning_rate=0.00025, buffer_size=1e6, discount=0.99):\n",
    "        super().__init__(env, random_seed)\n",
    "        self.net = DQNNetwork(self.act_dim).to(self.device)\n",
    "        self.target_net = deepcopy(self.net)\n",
    "        self.optimizer = torch.optim.RMSprop(params=self.net.parameters(), lr=learning_rate, momentum=0.95, eps=0.01)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.discount = discount\n",
    "        self.replay_start_frames = int(1e4)\n",
    "        self.minibatch_size = 32\n",
    "        self.init_expl = 1\n",
    "        self.fin_expl = 0.1\n",
    "        self.fin_expl_frame = int(3e4)\n",
    "        self._loss = []\n",
    "    \n",
    "    def _preprocessing(self, images):\n",
    "        return torch.Tensor(\n",
    "            [\n",
    "                rescale(np.dot(image, [0.299, 0.587, 0.114]), 0.5, multichannel=False)\n",
    "                 for image in images\n",
    "            ]\n",
    "        ).unsqueeze(0)\n",
    "    \n",
    "    def _step(self, action, render=False):\n",
    "        if render:\n",
    "            obs = []\n",
    "            for _ in range(4):\n",
    "                obs.append(self.env.step(action))\n",
    "                self.env.render()\n",
    "        else:\n",
    "            obs = [self.env.step(action) for _ in range(4)]\n",
    "        reward = np.sign(sum([ob[2] for ob in obs]))\n",
    "        done = obs[-1][2]\n",
    "        return self._preprocessing([ob[0] for ob in obs]), reward, done\n",
    "    \n",
    "    def _do_initial_warmup(self):\n",
    "        frames = 0\n",
    "        while True:\n",
    "            obs = self.env.reset()\n",
    "            observation = self._preprocessing([obs] * 4)\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _, _ = self.choose_action(observation, epsilon=1)\n",
    "                new_observation, reward, done = self._step(action)\n",
    "                self.replay_buffer.store(observation, action, new_observation, reward, done)\n",
    "                observation = new_observation\n",
    "                frames += 4\n",
    "                if frames % 1000 == 0:\n",
    "                    print(f\"initial frame {frames} completed\")\n",
    "                if frames > self.replay_start_frames:\n",
    "                    return\n",
    "            print(\"one full game has been played\")\n",
    "\n",
    "    def _do_minibatch(self, epsilon, init_obs):\n",
    "        observation = init_obs\n",
    "        for _ in range(8):\n",
    "            action, _, _ = self.choose_action(observation, epsilon)\n",
    "            new_observation, reward, done = self._step(action)\n",
    "            self.replay_buffer.store(observation, action, new_observation, reward, done)\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "                observation = self._preprocessing([obs] * 4)\n",
    "            else:\n",
    "                observation = new_observation\n",
    "        samples = self.replay_buffer.sample(self.minibatch_size)\n",
    "        loss = self.loss_function(samples)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return new_observation\n",
    "\n",
    "    def _do_exploration_phase(self):\n",
    "        obs = self.env.reset()\n",
    "        observation = self._preprocessing([obs] * 4)\n",
    "        for i in range(self.fin_expl_frame // 16):\n",
    "            epsilon = self.init_expl - ((i/(self.fin_expl_frame // 16)) * (self.init_expl - self.fin_expl))\n",
    "            if i % 100 == 0:\n",
    "                print(f\"starting exploration episode {i}, epsilon: {epsilon}, resetting target net\")\n",
    "                self.target_net = deepcopy(self.net)\n",
    "            observation = self._do_minibatch(epsilon, observation)\n",
    "        observation = self._do_minibatch(self.fin_expl, observation)\n",
    "        print(\"ended exploration\")\n",
    "\n",
    "    def _do_action_phase(self):\n",
    "        obs = self.env.reset()\n",
    "        observation = self._preprocessing([obs] * 4)\n",
    "        for i in range(self.fin_expl_frame // 16):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"starting action episode {i}, resetting target net\")\n",
    "                self.target_net = deepcopy(self.net)\n",
    "            observation = self._do_minibatch(self.fin_expl, observation)\n",
    "\n",
    "    def _set_lr(self, lr):\n",
    "        self.optimizer.defaults[\"lr\"] = lr\n",
    "    \n",
    "    def train(self, lr=None):\n",
    "        if lr:\n",
    "            self._set_lr(lr)\n",
    "        print(\"performing initial warmup\")\n",
    "        self._do_initial_warmup()\n",
    "        print(\"warmup complete\")\n",
    "        \n",
    "        print(\"starting exploration\")\n",
    "        self._do_exploration_phase()\n",
    "        print(\"exploration complete\")\n",
    "        \n",
    "        print(\"starting action at min epsilon\")\n",
    "        self._do_action_phase()\n",
    "        print(\"action finished\")\n",
    "    \n",
    "    def loss_function(self, samples):\n",
    "        sample_rewards = []\n",
    "        sample_q_values = []\n",
    "        for sample in samples:\n",
    "            sample_q_values.append(self.choose_action(sample.init_state)[2][0][sample.act.detach()])\n",
    "            with torch.no_grad():\n",
    "                sample_rewards.append(\n",
    "                    sample.reward\n",
    "                    if sample.terminal\n",
    "                    else sample.reward + self.discount * self.choose_action(sample.after_state, target_net=True)[1]\n",
    "                )\n",
    "        sample_q_values = torch.stack(sample_q_values)\n",
    "        sample_rewards = torch.as_tensor(sample_rewards, device=self.device, dtype=torch.float)\n",
    "        return torch.mean((sample_rewards - sample_q_values) ** 2)\n",
    "    \n",
    "    def choose_action(self, obs, epsilon=None, target_net=False):\n",
    "        if epsilon == 1:\n",
    "            return torch.as_tensor(np.random.choice(self.act_dim)).to(self.device), None, None\n",
    "        if epsilon and np.random.binomial(1, epsilon, 1)[0]:\n",
    "            return torch.as_tensor(np.random.choice(self.act_dim)).to(self.device), None, None\n",
    "        if target_net:\n",
    "            net = self.target_net\n",
    "        else:\n",
    "            net = self.net\n",
    "        act_vals = net(obs.to(self.device))\n",
    "        action = torch.argmax(act_vals)\n",
    "        return action, act_vals[0][action], act_vals\n",
    "    \n",
    "    def act(self, steps, render=True):\n",
    "        obs = self.env.reset()\n",
    "        observation = self._preprocessing([obs] * 4)\n",
    "        img = None\n",
    "        if render:\n",
    "            self.env.render()\n",
    "        try:\n",
    "            for i in range(steps):\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                action, _, _ = self.choose_action(observation, 1)\n",
    "                observation, _, done = self._step(action, render)\n",
    "                if done:\n",
    "                    break\n",
    "        finally:\n",
    "            self.env.close()\n",
    "dqn = DQN(\"BreakoutNoFrameskip-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing initial warmup\n",
      "one full game has been played\n",
      "initial frame 1000 completed\n",
      "one full game has been played\n",
      "initial frame 2000 completed\n",
      "one full game has been played\n",
      "initial frame 3000 completed\n",
      "one full game has been played\n",
      "one full game has been played\n",
      "initial frame 4000 completed\n",
      "one full game has been played\n",
      "one full game has been played\n",
      "initial frame 5000 completed\n",
      "one full game has been played\n",
      "initial frame 6000 completed\n",
      "one full game has been played\n",
      "one full game has been played\n",
      "initial frame 7000 completed\n",
      "one full game has been played\n",
      "initial frame 8000 completed\n",
      "one full game has been played\n",
      "initial frame 9000 completed\n",
      "one full game has been played\n",
      "initial frame 10000 completed\n",
      "warmup complete\n",
      "starting exploration\n",
      "starting exploration episode 0, epsilon: 1.0, resetting target net\n",
      "starting exploration episode 100, epsilon: 0.952, resetting target net\n",
      "starting exploration episode 200, epsilon: 0.904, resetting target net\n",
      "starting exploration episode 300, epsilon: 0.856, resetting target net\n",
      "starting exploration episode 400, epsilon: 0.808, resetting target net\n",
      "starting exploration episode 500, epsilon: 0.76, resetting target net\n",
      "starting exploration episode 600, epsilon: 0.712, resetting target net\n",
      "starting exploration episode 700, epsilon: 0.6639999999999999, resetting target net\n",
      "starting exploration episode 800, epsilon: 0.616, resetting target net\n",
      "starting exploration episode 900, epsilon: 0.5680000000000001, resetting target net\n",
      "starting exploration episode 1000, epsilon: 0.52, resetting target net\n",
      "starting exploration episode 1100, epsilon: 0.472, resetting target net\n",
      "starting exploration episode 1200, epsilon: 0.42399999999999993, resetting target net\n",
      "starting exploration episode 1300, epsilon: 0.376, resetting target net\n",
      "starting exploration episode 1400, epsilon: 0.32799999999999996, resetting target net\n",
      "starting exploration episode 1500, epsilon: 0.2799999999999999, resetting target net\n",
      "starting exploration episode 1600, epsilon: 0.23199999999999998, resetting target net\n",
      "starting exploration episode 1700, epsilon: 0.18400000000000005, resetting target net\n",
      "starting exploration episode 1800, epsilon: 0.136, resetting target net\n",
      "ended exploration\n",
      "exploration complete\n",
      "starting action at min epsilon\n",
      "starting action episode 0, resetting target net\n",
      "starting action episode 100, resetting target net\n",
      "starting action episode 200, resetting target net\n",
      "starting action episode 300, resetting target net\n",
      "starting action episode 400, resetting target net\n",
      "starting action episode 500, resetting target net\n",
      "starting action episode 600, resetting target net\n",
      "starting action episode 700, resetting target net\n",
      "starting action episode 800, resetting target net\n",
      "starting action episode 900, resetting target net\n",
      "starting action episode 1000, resetting target net\n",
      "starting action episode 1100, resetting target net\n",
      "starting action episode 1200, resetting target net\n",
      "starting action episode 1300, resetting target net\n",
      "starting action episode 1400, resetting target net\n",
      "starting action episode 1500, resetting target net\n",
      "starting action episode 1600, resetting target net\n",
      "starting action episode 1700, resetting target net\n",
      "starting action episode 1800, resetting target net\n",
      "action finished\n"
     ]
    }
   ],
   "source": [
    "dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.act(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32509"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.replay_buffer._index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [t[3] for t in dqn.replay_buffer._storage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "=====\n",
    "\n",
    "* target network\n",
    "* Make sure it's actually learning anything (more loggin, investigate tensorboard/pytorch specific solution?)\n",
    "* visualisation\n",
    "* assessing performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
