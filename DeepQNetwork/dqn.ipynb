{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of Deep Q-Learning\n",
    "As described in [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf), published in Nature, 26 February 2015. Research performed by DeepMind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robert/repos/DeepRLAlgos\n"
     ]
    }
   ],
   "source": [
    "cd /home/robert/repos/DeepRLAlgos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from time import sleep\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from common.algorithm import RLAlgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", [\"init_state\", \"act\", \"after_state\", \"reward\", \"terminal\"])\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self._size = size\n",
    "        self._storage = []\n",
    "        self._len = self._index = 0\n",
    "    \n",
    "    def store(self, init_state, act, after_state, reward, terminal):\n",
    "        if self._len == self._size:\n",
    "            self._storage[self._index] = Experience(init_state, act, after_state, reward, terminal)\n",
    "            self._index %= self._size\n",
    "        else:\n",
    "            self._storage.append(Experience(init_state, act, after_state, reward, terminal))\n",
    "            self._len = self._index = self._index + 1\n",
    "    \n",
    "    def sample(self, n_samples=1):\n",
    "        sample_indexes = np.random.choice(len(self._storage), n_samples, replace=False, )\n",
    "        return [self._storage[i] for i in sample_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(images):\n",
    "    return [np.mean(img[::2,::2], axis=2).astype(np.uint8) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=8, stride=4)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n",
    "        # self.conv_3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        # self.linear_1 = nn.Linear(in_features=3456, out_features=512)\n",
    "        self.linear_1 = nn.Linear(in_features=2816, out_features=256)\n",
    "        self.head = nn.Linear(in_features=256, out_features=action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        obs = F.relu(self.conv_1(obs))\n",
    "        obs = F.relu(self.conv_2(obs))\n",
    "#        obs = F.relu(self.conv_3(obs))\n",
    "        obs = obs.view(obs.size(0), -1) # resizing image to flat for linear\n",
    "        obs = F.relu(self.linear_1(obs))\n",
    "        return self.head(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(RLAlgorithm):\n",
    "    def __init__(self, env, random_seed=100, learning_rate=0.00025, buffer_size=1e6, discount=0.99):\n",
    "        super().__init__(env, random_seed)\n",
    "        self.net = DQNNetwork(self.act_dim).to(self.device)\n",
    "        self.target_net = deepcopy(self.net)\n",
    "        self.optimizer = torch.optim.RMSprop(params=self.net.parameters(), lr=learning_rate, momentum=0.95, eps=0.01)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.discount = discount\n",
    "        self.replay_start_frames = int(5e4)\n",
    "        self.minibatch_size = 64\n",
    "        self.init_expl = 1\n",
    "        self.fin_expl = 0.1\n",
    "        self.fin_expl_frame = int(2e5)\n",
    "        self.fin_frame = int(2e6)\n",
    "    \n",
    "    def _step(self, action, render=False):\n",
    "        if render:\n",
    "            obs = []\n",
    "            for _ in range(4):\n",
    "                obs.append(self.env.step(action))\n",
    "                self.env.render()\n",
    "        else:\n",
    "            obs = [self.env.step(action) for _ in range(4)]\n",
    "        reward = np.sum(sum([ob[2] for ob in obs]))\n",
    "        done = obs[-1][2]\n",
    "        return preprocess([ob[0] for ob in obs]), reward, done\n",
    "    \n",
    "    def _do_initial_warmup(self):\n",
    "        frames = 0\n",
    "        while True:\n",
    "            obs = self.env.reset()\n",
    "            observation = preprocess([obs] * 4)\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _, _ = self.choose_action(observation, epsilon=1)\n",
    "                new_observation, reward, done = self._step(action)\n",
    "                self.replay_buffer.store(observation, action, new_observation, reward, done)\n",
    "                observation = new_observation\n",
    "                frames += 4\n",
    "                if frames % 1000 == 0:\n",
    "                    print(f\"initial frame {frames} completed\")\n",
    "                if frames > self.replay_start_frames:\n",
    "                    return\n",
    "\n",
    "    def _do_minibatch(self, epsilon, init_obs):\n",
    "        observation = init_obs\n",
    "        for _ in range(8):\n",
    "            action, _, _ = self.choose_action(observation, epsilon)\n",
    "            new_observation, reward, done = self._step(action)\n",
    "            self.replay_buffer.store(observation, action, new_observation, reward, done)\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "                observation = preprocess([obs] * 4)\n",
    "            else:\n",
    "                observation = new_observation\n",
    "        samples = self.replay_buffer.sample(self.minibatch_size)\n",
    "        loss = self.loss_function(samples)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return new_observation\n",
    "\n",
    "    def _do_exploration_phase(self):\n",
    "        obs = self.env.reset()\n",
    "        observation = preprocess([obs] * 4)\n",
    "        frames = 0\n",
    "        while True:\n",
    "            if frames > self.fin_expl_frame:\n",
    "                print(f\"ended exploration, epsilon: {epsilon}\")\n",
    "                return\n",
    "            epsilon = self.init_expl - ((frames/(self.fin_expl_frame)) * (self.init_expl - self.fin_expl))\n",
    "            if frames % 3200 == 0:\n",
    "                print(f\"starting exploration frame {frames}, epsilon: {epsilon}\")\n",
    "            if frames != 0 and frames % 9984 == 0:\n",
    "                print(\"copying to target network\")\n",
    "                self.target_net = deepcopy(self.net)\n",
    "            observation = self._do_minibatch(epsilon, observation)\n",
    "            frames += 32\n",
    "\n",
    "    def _do_action_phase(self):\n",
    "        obs = self.env.reset()\n",
    "        observation = preprocess([obs] * 4)\n",
    "        frames = 0\n",
    "        while True:\n",
    "            if frames > self.fin_frame:\n",
    "                return\n",
    "            if i - 1 % 100 == 0:\n",
    "                print(f\"starting action episode {i}, resetting target net\")\n",
    "                self.target_net = deepcopy(self.net)\n",
    "            observation = self._do_minibatch(self.fin_expl, observation)\n",
    "\n",
    "    def _set_lr(self, lr):\n",
    "        self.optimizer.defaults[\"lr\"] = lr\n",
    "    \n",
    "    def train(self, lr=None):\n",
    "        if lr:\n",
    "            self._set_lr(lr)\n",
    "        print(\"performing initial warmup\")\n",
    "        self._do_initial_warmup()\n",
    "        print(\"warmup complete\")\n",
    "        \n",
    "        print(\"starting exploration\")\n",
    "        self._do_exploration_phase()\n",
    "        print(\"exploration complete\")\n",
    "        \n",
    "        print(\"starting action at min epsilon\")\n",
    "        self._do_action_phase()\n",
    "        print(\"action finished\")\n",
    "    \n",
    "    def loss_function(self, samples):\n",
    "        before_states = torch.as_tensor([s.init_state for s in samples], dtype=torch.float, device=self.device)\n",
    "        after_states = torch.as_tensor([s.after_state for s in samples], dtype=torch.float, device=self.device)\n",
    "        actions = torch.as_tensor([s.act for s in samples], dtype=torch.long, device=self.device)\n",
    "        rewards = torch.as_tensor([s.reward for s in samples], dtype=torch.float, device=self.device)\n",
    "        non_terminals = torch.as_tensor([not s.terminal for s in samples], dtype=torch.float, device=self.device)\n",
    "        \n",
    "        action_values = self.net(before_states)\n",
    "        q_values = action_values.gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            new_action_values = torch.max(self.target_net(after_states), dim=1)[0]\n",
    "            bootstrap_q_values = rewards + ((non_terminals) * self.discount * new_action_values)\n",
    "    \n",
    "        error = q_values - bootstrap_q_values\n",
    "        quadratic_term = (error**2)/ 2\n",
    "        linear_term = abs(error) - 1/2\n",
    "        use_linear_term = (abs(error) > 1.0).float()\n",
    "        return torch.mean(use_linear_term * linear_term + (1-use_linear_term) * quadratic_term)\n",
    "    \n",
    "    def choose_action(self, obs, epsilon=None, target_net=False):\n",
    "        if epsilon == 1:\n",
    "            return torch.as_tensor(np.random.choice(self.act_dim)).to(self.device), None, None\n",
    "        if epsilon and np.random.binomial(1, epsilon, 1)[0]:\n",
    "            return torch.as_tensor(np.random.choice(self.act_dim)).to(self.device), None, None\n",
    "        if target_net:\n",
    "            net = self.target_net\n",
    "        else:\n",
    "            net = self.net\n",
    "        obs = torch.as_tensor(obs, device=self.device).unsqueeze(0).float()\n",
    "        act_vals = net(obs.to(self.device))\n",
    "        action = int(torch.argmax(act_vals))\n",
    "        return action, act_vals[0][action], act_vals\n",
    "    \n",
    "    def act(self, steps, render=True):\n",
    "        obs = self.env.reset()\n",
    "        observation = preprocess([obs] * 4)\n",
    "        img = None\n",
    "        if render:\n",
    "            self.env.render()\n",
    "        try:\n",
    "            for i in range(steps):\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                    sleep(0.05)\n",
    "                action, _, _ = self.choose_action(observation, 1)\n",
    "                observation, _, done = self._step(action, render)\n",
    "                if done:\n",
    "                    break\n",
    "        finally:\n",
    "            self.env.close()\n",
    "dqn = DQN(\"BreakoutNoFrameskip-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing initial warmup\n",
      "initial frame 1000 completed\n",
      "initial frame 2000 completed\n",
      "initial frame 3000 completed\n",
      "initial frame 4000 completed\n",
      "initial frame 5000 completed\n",
      "initial frame 6000 completed\n",
      "initial frame 7000 completed\n",
      "initial frame 8000 completed\n",
      "initial frame 9000 completed\n",
      "initial frame 10000 completed\n",
      "initial frame 11000 completed\n",
      "initial frame 12000 completed\n",
      "initial frame 13000 completed\n",
      "initial frame 14000 completed\n",
      "initial frame 15000 completed\n",
      "initial frame 16000 completed\n",
      "initial frame 17000 completed\n",
      "initial frame 18000 completed\n",
      "initial frame 19000 completed\n",
      "initial frame 20000 completed\n",
      "initial frame 21000 completed\n",
      "initial frame 22000 completed\n",
      "initial frame 23000 completed\n",
      "initial frame 24000 completed\n",
      "initial frame 25000 completed\n",
      "initial frame 26000 completed\n",
      "initial frame 27000 completed\n",
      "initial frame 28000 completed\n",
      "initial frame 29000 completed\n",
      "initial frame 30000 completed\n",
      "initial frame 31000 completed\n",
      "initial frame 32000 completed\n",
      "initial frame 33000 completed\n",
      "initial frame 34000 completed\n",
      "initial frame 35000 completed\n",
      "initial frame 36000 completed\n",
      "initial frame 37000 completed\n",
      "initial frame 38000 completed\n",
      "initial frame 39000 completed\n",
      "initial frame 40000 completed\n",
      "initial frame 41000 completed\n",
      "initial frame 42000 completed\n",
      "initial frame 43000 completed\n",
      "initial frame 44000 completed\n",
      "initial frame 45000 completed\n",
      "initial frame 46000 completed\n",
      "initial frame 47000 completed\n",
      "initial frame 48000 completed\n",
      "initial frame 49000 completed\n",
      "initial frame 50000 completed\n",
      "warmup complete\n",
      "starting exploration\n",
      "starting exploration frame 0, epsilon: 1.0\n",
      "starting exploration frame 3200, epsilon: 0.9856\n"
     ]
    }
   ],
   "source": [
    "dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-911c0f3d64c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dqn' is not defined"
     ]
    }
   ],
   "source": [
    "dqn.act(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-86ebb7fee113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-cd040f5d0be2>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n_samples)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0msample_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "samples = dqn.replay_buffer.sample(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_loss_function(samples): \n",
    "    sample_rewards = []\n",
    "    sample_q_values = []\n",
    "    for sample in samples:\n",
    "        sample_q_values.append(dqn.choose_action(sample.init_state)[2][0][sample.act])\n",
    "        with torch.no_grad():\n",
    "            sample_rewards.append(\n",
    "                sample.reward\n",
    "                if sample.terminal\n",
    "                else sample.reward + dqn.discount * dqn.choose_action(sample.after_state, target_net=True)[1]\n",
    "            )\n",
    "    sample_q_values = torch.stack(sample_q_values)\n",
    "    sample_rewards = torch.as_tensor(sample_rewards, device=dqn.device, dtype=torch.float)\n",
    "    return torch.mean(sample_rewards - sample_q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward1>),\n",
       " tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = dqn.replay_buffer.sample(32)\n",
    "old_loss_function(samples), dqn.loss_function(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "=====\n",
    "\n",
    "* target network\n",
    "* Make sure it's actually learning anything (more loggin, investigate tensorboard/pytorch specific solution?)\n",
    "* visualisation\n",
    "* assessing performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
